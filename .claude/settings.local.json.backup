{
  "permissions": {
    "allow": [
      "mcp__filesystem__directory_tree",
      "mcp__filesystem__list_directory_with_sizes",
      "mcp__filesystem__read_text_file",
      "Bash(pytest:*)",
      "Bash(python -m mypy:*)",
      "Bash(python3 -m mypy:*)",
      "Bash(python3 -m ruff check:*)",
      "Bash(git add:*)",
      "Bash(git commit:*)",
      "Bash(python -m pytest:*)",
      "Bash(python3 -m pytest:*)",
      "Bash(python3 -m pylint:*)",
      "Bash(python3:*)",
      "Bash(ruff check:*)",
      "Bash(pip install:*)",
      "Bash(pip3 install:*)",
      "Bash(python:*)",
      "Bash(mypy:*)",
      "Bash(ruff format:*)",
      "mcp__filesystem__list_allowed_directories",
      "mcp__filesystem__list_directory",
      "Bash(git push:*)",
      "Bash(supabase db push:*)",
      "Bash(supabase link:*)",
      "Bash(supabase db pull:*)",
      "Bash(supabase migration repair:*)",
      "Bash(supabase migration list:*)",
      "Bash(supabase db remote:*)",
      "Bash(supabase db execute:*)",
      "Bash(psql:*)",
      "Bash(supabase start:*)",
      "Bash(supabase db dump:*)",
      "Bash(git log:*)",
      "Bash(git show:*)",
      "Bash(PYTHONPATH=/Users/dhruv/aria/backend pytest:*)",
      "Bash(PYTHONPATH=. python3:*)",
      "Bash(PYTHONPATH=/Users/dhruv/aria python3:*)",
      "Bash(npm run typecheck:*)",
      "Bash(pip3:*)",
      "WebSearch",
      "Bash(npm run lint:*)",
      "Bash(pip show:*)",
      "Bash(pip index versions composio)",
      "Bash(pip index versions:*)",
      "Bash(npm run build:*)",
      "Bash(npx eslint:*)",
      "Bash(ls:*)",
      "Bash(PYTHONPATH=/Users/dhruv/aria/backend/src pytest:*)",
      "Bash(grep:*)",
      "Bash(npm install:*)",
      "Bash(npx supabase db push:*)",
      "Bash(npx supabase db remote:*)",
      "Bash(npx supabase migration up:*)",
      "Bash(git -C /Users/dhruv/aria add frontend/src/pages/AriaChat.tsx)",
      "Bash(git -C /Users/dhruv/aria commit:*)",
      "Bash(git remote set-url:*)",
      "Bash(git pull:*)",
      "Bash(git stash:*)",
      "mcp__filesystem__create_directory",
      "Bash(supabase db reset:*)",
      "Bash(gh issue list:*)",
      "Bash(PYTHONPATH=/Users/dhruv/aria/backend python3 -c \"\nfrom datetime import datetime, UTC\nfrom src.db.supabase import SupabaseClient\nfrom src.memory.lead_memory_events import LeadEventService\nfrom src.models.lead_memory import Direction, EventType, LeadEventCreate\n\n# Test the exact pattern from the docstring\nprint\\(''Testing docstring example types...''\\)\nprint\\(f''Direction.OUTBOUND = {Direction.OUTBOUND}''\\)\nprint\\(f''EventType.EMAIL_SENT = {EventType.EMAIL_SENT}''\\)\nprint\\(f''LeadEventCreate fields: {LeadEventCreate.model_fields.keys\\(\\)}''\\)\nprint\\(''All types from docstring are valid!''\\)\n\")",
      "Bash(PYTHONPATH=/Users/dhruv/aria/backend python:*)",
      "Bash(PYTHONPATH=/Users/dhruv/aria/backend python3:*)",
      "Bash(PYTHONPATH=src python3:*)",
      "Bash(git -C /Users/dhruv/aria add:*)",
      "Bash(git -C /Users/dhruv/aria log -1 --stat)",
      "Bash(git -C /Users/dhruv/aria log:*)",
      "Bash(/Library/Frameworks/Python.framework/Versions/3.14/bin/python3:*)",
      "Bash(/Users/dhruv/aria/backend/.venv/bin/python -m pytest:*)",
      "Bash(supabase migration:*)",
      "mcp__supabase__query",
      "Bash(supabase status:*)",
      "Bash(\" 2>&1 || echo \"Could not verify table exists, but migration 005 is applied \")",
      "mcp__filesystem__get_file_info",
      "mcp__filesystem__write_file",
      "Bash(supabase db inspect:*)",
      "Bash(git -C /Users/dhruv/aria status:*)",
      "Bash(git -C /Users/dhruv/aria diff --stat)",
      "Bash(git -C /Users/dhruv/aria diff backend/src/security/skill_audit.py backend/tests/test_skill_audit.py)",
      "Bash(source:*)",
      "Bash(supabase --version:*)",
      "Bash(npx supabase migration list:*)",
      "Bash(npx supabase db pull:*)",
      "Bash(npx supabase migration repair:*)",
      "Bash(npx supabase:*)",
      "Bash(/Users/dhruv/aria/backend/tests/test_activation.py << 'EOF'\n\"\"\"Tests for US-915: Onboarding Completion → Agent Activation.\"\"\"\n\nimport pytest\nfrom unittest.mock import AsyncMock, MagicMock, patch\n\nfrom src.onboarding.activation import OnboardingCompletionOrchestrator\n\n\n@pytest.fixture\ndef mock_db\\(\\):\n    \"\"\"Mock Supabase client.\"\"\"\n    return MagicMock\\(\\)\n\n\n@pytest.fixture\ndef mock_llm\\(\\):\n    \"\"\"Mock LLM client.\"\"\"\n    return MagicMock\\(\\)\n\n\n@pytest.fixture\ndef mock_goal_service\\(\\):\n    \"\"\"Mock GoalService.\"\"\"\n    service = MagicMock\\(\\)\n    service.create_goal = AsyncMock\\(\\)\n    return service\n\n\n@pytest.fixture\ndef activator\\(mock_db, mock_llm, mock_goal_service\\):\n    \"\"\"Create OnboardingCompletionOrchestrator with mocked dependencies.\"\"\"\n    with patch\\(\"src.onboarding.activation.SupabaseClient.get_client\", return_value=mock_db\\):\n        with patch\\(\"src.onboarding.activation.LLMClient\", return_value=mock_llm\\):\n            with patch\\(\"src.onboarding.activation.GoalService\", return_value=mock_goal_service\\):\n                return OnboardingCompletionOrchestrator\\(\\)\n\n\nclass TestOnboardingCompletionOrchestrator:\n    \"\"\"Test suite for OnboardingCompletionOrchestrator.\"\"\"\n\n    @pytest.mark.asyncio\n    async def test_activate_all_agents_with_crm_and_lead_gen\\(\n        self, activator, mock_goal_service\n    \\):\n        \"\"\"Test activation with CRM connected and lead_gen goal.\"\"\"\n        # Setup mock responses\n        mock_goal_service.create_goal.side_effect = [\n            {\"id\": \"goal-1\"},  # Scout\n            {\"id\": \"goal-2\"},  # Analyst\n            {\"id\": \"goal-3\"},  # Hunter\n            {\"id\": \"goal-4\"},  # Operator\n            {\"id\": \"goal-5\"},  # Scribe\n        ]\n\n        onboarding_data = {\n            \"company_id\": \"company-123\",\n            \"company_discovery\": {\"website\": \"example.com\"},\n            \"enrichment\": {\"competitors\": [\"competitor1.com\", \"competitor2.com\"]},\n            \"integration_wizard\": {\n                \"crm_connected\": True,\n                \"email_connected\": True,\n            },\n            \"first_goal\": {\"goal_type\": \"lead_gen\", \"description\": \"Generate 50 leads\"},\n        }\n\n        with patch.object\\(\n            activator, \"_record_activation_event\", new_callable=AsyncMock\n        \\):\n            result = await activator.activate\\(\"user-123\", onboarding_data\\)\n\n        # Verify all agents were activated\n        assert result[\"user_id\"] == \"user-123\"\n        assert \"activated_at\" in result\n        assert result[\"activations\"][\"scout\"] is not None\n        assert result[\"activations\"][\"analyst\"] is not None\n        assert result[\"activations\"][\"hunter\"] is not None\n        assert result[\"activations\"][\"operator\"] is not None\n        assert result[\"activations\"][\"scribe\"] is not None\n\n        # Verify goals were created\n        assert mock_goal_service.create_goal.call_count == 5\n\n    @pytest.mark.asyncio\n    async def test_activate_without_crm_skips_analyst_operator\\(\n        self, activator, mock_goal_service\n    \\):\n        \"\"\"Test activation without CRM skips Analyst and Operator.\"\"\"\n        mock_goal_service.create_goal.side_effect = [\n            {\"id\": \"goal-1\"},  # Scout\n            {\"id\": \"goal-2\"},  # Scribe \\(email not connected either\\)\n        ]\n\n        onboarding_data = {\n            \"company_id\": \"company-123\",\n            \"company_discovery\": {\"website\": \"example.com\"},\n            \"enrichment\": {},\n            \"integration_wizard\": {\n                \"crm_connected\": False,\n                \"email_connected\": False,\n            },\n            \"first_goal\": {\"goal_type\": \"research\"},\n        }\n\n        with patch.object\\(\n            activator, \"_record_activation_event\", new_callable=AsyncMock\n        \\):\n            result = await activator.activate\\(\"user-123\", onboarding_data\\)\n\n        # Verify Scout still activated\n        assert result[\"activations\"][\"scout\"] is not None\n        # Verify Analyst and Operator skipped\n        assert result[\"activations\"][\"analyst\"] is None\n        assert result[\"activations\"][\"operator\"] is None\n        assert result[\"activations\"][\"scribe\"] is None\n        # Hunter not activated \\(no lead_gen goal\\)\n        assert result[\"activations\"][\"hunter\"] is None\n\n        # Only Scout goal created\n        assert mock_goal_service.create_goal.call_count == 1\n\n    @pytest.mark.asyncio\n    async def test_activate_scout_uses_competitors_from_enrichment\\(\n        self, activator, mock_goal_service\n    \\):\n        \"\"\"Test Scout activation uses competitors from enrichment.\"\"\"\n        mock_goal_service.create_goal.return_value = {\"id\": \"goal-1\"}\n\n        onboarding_data = {\n            \"company_id\": \"company-123\",\n            \"company_discovery\": {\"website\": \"mycompany.com\"},\n            \"enrichment\": {\n                \"competitors\": [\"comp1.com\", \"comp2.com\", \"comp3.com\", \"comp4.com\"]\n            },\n            \"integration_wizard\": {\"crm_connected\": False},\n            \"first_goal\": {\"goal_type\": \"research\"},\n        }\n\n        with patch.object\\(\n            activator, \"_record_activation_event\", new_callable=AsyncMock\n        \\):\n            await activator.activate\\(\"user-123\", onboarding_data\\)\n\n        # Verify Scout goal was created with competitors\n        call_args = mock_goal_service.create_goal.call_args\n        goal_data = call_args[0][1]  # Second positional arg is GoalCreate\n\n        assert goal_data.config[\"entities\"] == [\n            \"comp1.com\",\n            \"comp2.com\",\n            \"comp3.com\",\n            \"comp4.com\",\n        ]\n        assert goal_data.config[\"agent\"] == \"scout\"\n\n    @pytest.mark.asyncio\n    async def test_activate_scout_fallback_to_company_domain\\(\n        self, activator, mock_goal_service\n    \\):\n        \"\"\"Test Scout activation falls back to company domain without competitors.\"\"\"\n        mock_goal_service.create_goal.return_value = {\"id\": \"goal-1\"}\n\n        onboarding_data = {\n            \"company_id\": \"company-123\",\n            \"company_discovery\": {\"website\": \"mycompany.com\"},\n            \"enrichment\": {},  # No competitors\n            \"integration_wizard\": {\"crm_connected\": False},\n            \"first_goal\": {\"goal_type\": \"research\"},\n        }\n\n        with patch.object\\(\n            activator, \"_record_activation_event\", new_callable=AsyncMock\n        \\):\n            await activator.activate\\(\"user-123\", onboarding_data\\)\n\n        # Verify Scout goal was created with company domain as fallback\n        call_args = mock_goal_service.create_goal.call_args\n        goal_data = call_args[0][1]\n\n        assert goal_data.config[\"entities\"] == [\"mycompany.com\"]\n\n    @pytest.mark.asyncio\n    async def test_activate_without_email_skips_scribe\\(\n        self, activator, mock_goal_service\n    \\):\n        \"\"\"Test activation without email skips Scribe.\"\"\"\n        mock_goal_service.create_goal.side_effect = [\n            {\"id\": \"goal-1\"},  # Scout\n            {\"id\": \"goal-2\"},  # Analyst \\(CRM connected\\)\n            {\"id\": \"goal-3\"},  # Operator \\(CRM connected\\)\n        ]\n\n        onboarding_data = {\n            \"company_id\": \"company-123\",\n            \"company_discovery\": {\"website\": \"example.com\"},\n            \"enrichment\": {},\n            \"integration_wizard\": {\n                \"crm_connected\": True,\n                \"email_connected\": False,  # No email\n            },\n            \"first_goal\": {\"goal_type\": \"research\"},\n        }\n\n        with patch.object\\(\n            activator, \"_record_activation_event\", new_callable=AsyncMock\n        \\):\n            result = await activator.activate\\(\"user-123\", onboarding_data\\)\n\n        # Verify Scribe skipped\n        assert result[\"activations\"][\"scribe\"] is None\n        # Others activated\n        assert result[\"activations\"][\"scout\"] is not None\n        assert result[\"activations\"][\"analyst\"] is not None\n        assert result[\"activations\"][\"operator\"] is not None\n\n    @pytest.mark.asyncio\n    async def test_activate_without_lead_gen_skips_hunter\\(\n        self, activator, mock_goal_service\n    \\):\n        \"\"\"Test activation without lead_gen goal skips Hunter.\"\"\"\n        mock_goal_service.create_goal.side_effect = [\n            {\"id\": \"goal-1\"},\n        ] * 4  # Scout, Analyst, Operator, Scribe\n\n        onboarding_data = {\n            \"company_id\": \"company-123\",\n            \"company_discovery\": {\"website\": \"example.com\"},\n            \"enrichment\": {},\n            \"integration_wizard\": {\n                \"crm_connected\": True,\n                \"email_connected\": True,\n            },\n            \"first_goal\": {\"goal_type\": \"research\"},  # Not lead_gen\n        }\n\n        with patch.object\\(\n            activator, \"_record_activation_event\", new_callable=AsyncMock\n        \\):\n            result = await activator.activate\\(\"user-123\", onboarding_data\\)\n\n        # Verify Hunter skipped\n        assert result[\"activations\"][\"hunter\"] is None\n        # Others activated\n        assert result[\"activations\"][\"scout\"] is not None\n\n    @pytest.mark.asyncio\n    async def test_activate_handles_agent_failure_gracefully\\(\n        self, activator, mock_goal_service\n    \\):\n        \"\"\"Test activation continues even if one agent fails.\"\"\"\n        # Scout succeeds, Analyst fails, rest succeed\n        mock_goal_service.create_goal.side_effect = [\n            {\"id\": \"goal-1\"},  # Scout succeeds\n            Exception\\(\"Analyst failed\"\\),  # Analyst fails\n            {\"id\": \"goal-3\"},  # Hunter succeeds\n            {\"id\": \"goal-4\"},  # Operator succeeds\n            {\"id\": \"goal-5\"},  # Scribe succeeds\n        ]\n\n        onboarding_data = {\n            \"company_id\": \"company-123\",\n            \"company_discovery\": {\"website\": \"example.com\"},\n            \"enrichment\": {},\n            \"integration_wizard\": {\n                \"crm_connected\": True,\n                \"email_connected\": True,\n            },\n            \"first_goal\": {\"goal_type\": \"lead_gen\"},\n        }\n\n        with patch.object\\(\n            activator, \"_record_activation_event\", new_callable=AsyncMock\n        \\):\n            # Should not raise exception\n            result = await activator.activate\\(\"user-123\", onboarding_data\\)\n\n        # Verify other agents still activated\n        assert result[\"activations\"][\"scout\"] is not None\n        assert result[\"activations\"][\"analyst\"] is None  # Failed\n        assert result[\"activations\"][\"hunter\"] is not None\n        assert result[\"activations\"][\"operator\"] is not None\n        assert result[\"activations\"][\"scribe\"] is not None\n\n    @pytest.mark.asyncio\n    async def test_activate_records_episodic_event\\(\n        self, activator, mock_goal_service\n    \\):\n        \"\"\"Test activation records event to episodic memory.\"\"\"\n        mock_goal_service.create_goal.side_effect = [{\"id\": \"goal-1\"}]\n\n        onboarding_data = {\n            \"company_id\": \"company-123\",\n            \"company_discovery\": {\"website\": \"example.com\"},\n            \"enrichment\": {},\n            \"integration_wizard\": {\"crm_connected\": False},\n            \"first_goal\": {\"goal_type\": \"research\"},\n        }\n\n        mock_episodic_memory = MagicMock\\(\\)\n        mock_episodic_memory.store_episode = AsyncMock\\(\\)\n\n        with patch\\(\n            \"src.onboarding.activation.EpisodicMemory\", return_value=mock_episodic_memory\n        \\):\n            await activator.activate\\(\"user-123\", onboarding_data\\)\n\n        # Verify episodic event was recorded\n        assert mock_episodic_memory.store_episode.call_count == 1\n        episode_arg = mock_episodic_memory.store_episode.call_args[0][0]\n        assert episode_arg.event_type == \"onboarding_activation\"\n        assert \"Post-onboarding activation\" in episode_arg.content\n\n    @pytest.mark.asyncio\n    async def test_activate_goals_have_correct_source_and_priority\\(\n        self, activator, mock_goal_service\n    \\):\n        \"\"\"Test all created goals have onboarding_activation source and low priority.\"\"\"\n        mock_goal_service.create_goal.return_value = {\"id\": \"goal-1\"}\n\n        onboarding_data = {\n            \"company_id\": \"company-123\",\n            \"company_discovery\": {\"website\": \"example.com\"},\n            \"enrichment\": {},\n            \"integration_wizard\": {\"crm_connected\": True, \"email_connected\": True},\n            \"first_goal\": {\"goal_type\": \"lead_gen\"},\n        }\n\n        with patch.object\\(\n            activator, \"_record_activation_event\", new_callable=AsyncMock\n        \\):\n            await activator.activate\\(\"user-123\", onboarding_data\\)\n\n        # Verify each goal has correct config\n        for call in mock_goal_service.create_goal.call_args_list:\n            goal_data = call[0][1]  # Second positional arg is GoalCreate\n            assert goal_data.config[\"source\"] == \"onboarding_activation\"\n            assert goal_data.config[\"priority\"] == \"low\"\n            assert \"agent\" in goal_data.config\nEOF)",
      "Bash(PGPASSWORD=$SUPABASE_DB_PASSWORD psql:*)",
      "Bash(env:*)",
      "Bash(curl:*)",
      "Bash(PGPASSWORD='URh!4v.$jBXc4MT' psql:*)",
      "Bash(~/.pgpass)",
      "Bash(chmod:*)",
      "Bash(npx tsc:*)",
      "Bash(npm test:*)",
      "Bash(npm run test:*)",
      "Bash(/Users/dhruv/aria/docs/us-930-task-7-implementation-summary.md << 'EOF'\n# US-930 Task 7: Enhanced API Client with Retry Logic - Implementation Summary\n\n## Overview\n\nImplemented exponential backoff retry logic and event-based error notification system for the frontend API client.\n\n## Files Created\n\n### 1. Enhanced API Client\n**File:** `/Users/dhruv/aria/frontend/src/api/client.ts`\n\n**Features:**\n- Exponential backoff retry \\(1s, 2s, 4s delays\\)\n- Max retries: 3\n- Retries on status codes: 408, 429, 500, 502, 503, 504\n- Honors `retry-after` header for rate limiting\n- 401 handling with token refresh and redirect to login\n- Network error retry support\n- Integration with error event system for user notifications\n\n**Key Constants:**\n- `MAX_RETRIES = 3`\n- `BASE_DELAY = 1000ms`\n\n### 2. Error Event System\n**File:** `/Users/dhruv/aria/frontend/src/lib/errorEvents.ts`\n\n**Features:**\n- Event-based error notification system \\(works outside React render cycle\\)\n- `showError\\(type, title, description\\)` function to trigger notifications\n- `onError\\(callback\\)` subscription function with unsubscribe\n- Error type categorization: auth, network, server, client, retry, rate_limit, permission, not_found, validation\n- Color mapping for each error type\n- Auto-dismiss delay configuration per error type\n\n### 3. Error Toaster Component\n**File:** `/Users/dhruv/aria/frontend/src/components/ErrorToaster.tsx`\n\n**Features:**\n- Displays error notifications from the event system\n- Fixed position \\(bottom-right\\)\n- Max 3 toasts visible at once\n- Auto-dismiss with progress bar\n- Type-appropriate icons and colors\n- Smooth animations with Framer Motion\n- Manual dismiss button\n\n### 4. API Client Tests\n**File:** `/Users/dhruv/aria/frontend/src/api/__tests__/client.test.ts`\n\n**Coverage:**\n- Client configuration validation\n- Interceptor registration\n- Retry configuration \\(MAX_RETRIES, BASE_DELAY\\)\n- Exponential backoff calculation\n- Retryable status codes\n- Error type categorization\n- retry-after header parsing\n- Token refresh configuration\n\n**Result:** 18 tests passing\n\n### 5. Error Events Tests\n**File:** `/Users/dhruv/aria/frontend/src/lib/__tests__/errorEvents.test.ts`\n\n**Coverage:**\n- Error event structure\n- Listener registration/unregistration\n- Multiple listener support\n- Error handling in listeners\n- Icon and color mappings\n- Dismiss delay configuration\n- Unique ID generation\n\n**Result:** 17 tests passing\n\n## Files Modified\n\n### 1. App.tsx\n**File:** `/Users/dhruv/aria/frontend/src/App.tsx`\n\n**Changes:**\n- Added import for ErrorToaster component\n- Registered ErrorToaster in component tree\n\n## Implementation Details\n\n### Retry Logic Flow\n```\nRequest fails → Check if retryable → \n  ├─ Yes: Calculate delay \\(exponential backoff\\) → Wait → Retry\n  └─ No: Show error notification → Reject promise\n```\n\n### Error Handling Flow\n```\nError Response → Check status code →\n  ├─ 401: Attempt token refresh → Redirect to login if failed\n  ├─ 429: Honor retry-after header → Show rate limit notification\n  ├─ 5xx: Show server error notification\n  └─ Other: Show appropriate error notification\n```\n\n### Event-Based Notification Pattern\nSince React hooks can't be used in axios interceptors:\n1. Create event system \\(`errorEvents.ts`\\) with pub/sub pattern\n2. Interceptor calls `showError\\(\\)` to emit events\n3. ErrorToaster component subscribes to events and renders toasts\n\n## Quality Gates Passed\n\n- ✅ All new tests passing \\(35 tests total\\)\n- ✅ No linting errors in new files\n- ✅ TypeScript type checking valid for new files\n- ✅ Integration with existing codebase verified\n\n## Error Type Mappings\n\n| Type | Icon | Color | Use Case |\n|------|------|-------|----------|\n| auth | Lock | Amber | Authentication failures |\n| network | WifiOff | Orange | Network connectivity issues |\n| server | Server | Red | Server errors \\(5xx\\) |\n| client | AlertTriangle | Red | Client errors \\(4xx\\) |\n| retry | RefreshCw | Blue | Retry in progress |\n| rate_limit | Clock | Purple | Rate limiting \\(429\\) |\n| permission | ShieldAlert | Amber | Access denied \\(403\\) |\n| not_found | SearchX | Slate | Resource not found \\(404\\) |\n| validation | AlertCircle | Yellow | Validation errors |\n\n## Future Enhancements\n\n1. Add telemetry for monitoring retry success rates\n2. Configurable retry delays per endpoint\n3. Request deduplication during retry\n4. Offline queue for failed requests\n5. Detailed error analytics dashboard\nEOF)",
      "Bash(node:*)",
      "Bash(npx tsx --test:*)",
      "Bash(git status:*)",
      "Bash(git ls-tree:*)",
      "Bash(git fetch:*)",
      "Bash(git checkout:*)",
      "Bash(git rm:*)",
      "Bash(xargs git rm:*)",
      "Bash(npm run test)",
      "Bash(npm run lint)",
      "Bash(supabase:*)",
      "Bash(mymy:*)",
      "Bash(timeout:*)",
      "Bash(echo:*)",
      "Bash(test:*)",
      "Bash(git commit -m \"$\\(cat <<''EOF''\ntest: US-928 add billing service edge case tests\n\nCo-Authored-By: Claude Opus 4.6 <noreply@anthropic.com>\nEOF\n\\)\")",
      "Bash(npx vitest:*)",
      "Bash(git commit -m \"$\\(cat <<''EOF''\nfeat: US-933 integrate FeedbackWidget into ARIA chat messages\n\nCo-Authored-By: Claude Opus 4.6 <noreply@anthropic.com>\nEOF\n\\)\")",
      "Bash(cd /Users/dhruv/aria && ruff check:*)",
      "Bash(git push)"
    ]
  }
}
