# US-301: OODA Loop Implementation Plan

> **For Claude:** REQUIRED SUB-SKILL: Use superpowers:executing-plans to implement this plan task-by-task.

**Goal:** Implement OODA loop cognitive processing so ARIA can reason systematically about tasks.

**Architecture:** The OODA loop (Observe-Orient-Decide-Act) is ARIA's cognitive processing framework. Each cycle gathers context from memory (Observe), analyzes patterns (Orient), selects actions (Decide), and executes (Act). The loop iterates until the goal is achieved or max iterations reached. Each phase is logged for transparency with configurable thinking budgets.

**Tech Stack:** Python 3.11+, asyncio, dataclasses, LLMClient (Claude API), EpisodicMemory, SemanticMemory, WorkingMemory

---

## Task 1: Add OODA Exception Types

**Files:**
- Modify: `backend/src/core/exceptions.py`
- Test: `backend/tests/test_exceptions.py`

**Step 1: Write the failing test**

Add to `backend/tests/test_exceptions.py`:

```python
def test_ooda_loop_error_initializes_correctly() -> None:
    """Test OODALoopError initializes with proper attributes."""
    from src.core.exceptions import OODALoopError

    error = OODALoopError("Observation failed")
    assert error.message == "OODA loop error: Observation failed"
    assert error.code == "OODA_LOOP_ERROR"
    assert error.status_code == 500


def test_ooda_max_iterations_error_initializes_correctly() -> None:
    """Test OODAMaxIterationsError initializes with iteration count."""
    from src.core.exceptions import OODAMaxIterationsError

    error = OODAMaxIterationsError(goal_id="goal-123", iterations=10)
    assert "goal-123" in error.message
    assert "10" in error.message
    assert error.code == "OODA_MAX_ITERATIONS"
    assert error.status_code == 400
    assert error.details["goal_id"] == "goal-123"
    assert error.details["iterations"] == 10


def test_ooda_blocked_error_initializes_correctly() -> None:
    """Test OODABlockedError initializes with reason."""
    from src.core.exceptions import OODABlockedError

    error = OODABlockedError(goal_id="goal-123", reason="No available actions")
    assert "goal-123" in error.message
    assert "No available actions" in error.message
    assert error.code == "OODA_BLOCKED"
    assert error.status_code == 400
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_exceptions.py::test_ooda_loop_error_initializes_correctly -v`
Expected: FAIL with "ImportError: cannot import name 'OODALoopError'"

**Step 3: Write minimal implementation**

Add to `backend/src/core/exceptions.py` (after `CorporateFactNotFoundError`):

```python
class OODALoopError(ARIAException):
    """OODA loop processing error (500).

    Used for failures during OODA loop execution phases.
    """

    def __init__(self, message: str = "Unknown error") -> None:
        """Initialize OODA loop error.

        Args:
            message: Error details.
        """
        super().__init__(
            message=f"OODA loop error: {message}",
            code="OODA_LOOP_ERROR",
            status_code=500,
        )


class OODAMaxIterationsError(ARIAException):
    """OODA loop exceeded maximum iterations (400).

    Raised when the OODA loop cannot achieve the goal within
    the configured maximum number of iterations.
    """

    def __init__(self, goal_id: str, iterations: int) -> None:
        """Initialize max iterations error.

        Args:
            goal_id: The goal that could not be achieved.
            iterations: Number of iterations attempted.
        """
        super().__init__(
            message=f"OODA loop for goal '{goal_id}' exceeded maximum iterations ({iterations})",
            code="OODA_MAX_ITERATIONS",
            status_code=400,
            details={"goal_id": goal_id, "iterations": iterations},
        )


class OODABlockedError(ARIAException):
    """OODA loop is blocked and cannot proceed (400).

    Raised when the OODA loop cannot find any viable actions
    to take toward the goal.
    """

    def __init__(self, goal_id: str, reason: str) -> None:
        """Initialize blocked error.

        Args:
            goal_id: The goal that is blocked.
            reason: Why the loop is blocked.
        """
        super().__init__(
            message=f"OODA loop for goal '{goal_id}' is blocked: {reason}",
            code="OODA_BLOCKED",
            status_code=400,
            details={"goal_id": goal_id, "reason": reason},
        )
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_exceptions.py -k "ooda" -v`
Expected: PASS (3 tests)

**Step 5: Commit**

```bash
git add backend/src/core/exceptions.py backend/tests/test_exceptions.py
git commit -m "feat(ooda): add OODA loop exception types

- OODALoopError for general processing failures
- OODAMaxIterationsError when goal not achieved in max iterations
- OODABlockedError when no viable actions available"
```

---

## Task 2: Create OODAPhase Enum and OODAState Dataclass

**Files:**
- Create: `backend/src/core/ooda.py`
- Test: `backend/tests/test_ooda.py`

**Step 1: Write the failing test**

Create `backend/tests/test_ooda.py`:

```python
"""Tests for OODA loop cognitive processing module."""

import pytest
from datetime import datetime, UTC


def test_ooda_phase_enum_has_four_phases() -> None:
    """Test OODAPhase enum has observe, orient, decide, act."""
    from src.core.ooda import OODAPhase

    assert OODAPhase.OBSERVE.value == "observe"
    assert OODAPhase.ORIENT.value == "orient"
    assert OODAPhase.DECIDE.value == "decide"
    assert OODAPhase.ACT.value == "act"
    assert len(OODAPhase) == 4


def test_ooda_state_initializes_with_defaults() -> None:
    """Test OODAState initializes with sensible defaults."""
    from src.core.ooda import OODAState, OODAPhase

    state = OODAState(goal_id="goal-123")

    assert state.goal_id == "goal-123"
    assert state.current_phase == OODAPhase.OBSERVE
    assert state.observations == []
    assert state.orientation == {}
    assert state.decision is None
    assert state.action_result is None
    assert state.iteration == 0
    assert state.max_iterations == 10
    assert state.phase_logs == []
    assert state.is_complete is False
    assert state.is_blocked is False
    assert state.blocked_reason is None


def test_ooda_state_to_dict_serializes_correctly() -> None:
    """Test OODAState.to_dict produces valid dictionary."""
    from src.core.ooda import OODAState, OODAPhase

    state = OODAState(goal_id="goal-123", max_iterations=5)
    state.current_phase = OODAPhase.ORIENT
    state.observations = [{"source": "memory", "data": "test"}]
    state.iteration = 2

    result = state.to_dict()

    assert result["goal_id"] == "goal-123"
    assert result["current_phase"] == "orient"
    assert result["observations"] == [{"source": "memory", "data": "test"}]
    assert result["iteration"] == 2
    assert result["max_iterations"] == 5


def test_ooda_state_from_dict_deserializes_correctly() -> None:
    """Test OODAState.from_dict restores state."""
    from src.core.ooda import OODAState, OODAPhase

    data = {
        "goal_id": "goal-456",
        "current_phase": "decide",
        "observations": [{"source": "test"}],
        "orientation": {"patterns": ["pattern1"]},
        "decision": {"action": "search"},
        "action_result": None,
        "iteration": 3,
        "max_iterations": 10,
        "phase_logs": [],
        "is_complete": False,
        "is_blocked": False,
        "blocked_reason": None,
    }

    state = OODAState.from_dict(data)

    assert state.goal_id == "goal-456"
    assert state.current_phase == OODAPhase.DECIDE
    assert state.observations == [{"source": "test"}]
    assert state.iteration == 3


def test_ooda_phase_log_entry_captures_phase_execution() -> None:
    """Test OODAPhaseLogEntry captures phase execution details."""
    from src.core.ooda import OODAPhaseLogEntry, OODAPhase

    entry = OODAPhaseLogEntry(
        phase=OODAPhase.OBSERVE,
        iteration=1,
        input_summary="Query memory for context",
        output_summary="Found 5 relevant episodes",
        tokens_used=150,
        duration_ms=230,
    )

    assert entry.phase == OODAPhase.OBSERVE
    assert entry.iteration == 1
    assert entry.tokens_used == 150
    assert entry.duration_ms == 230
    assert entry.timestamp is not None


def test_ooda_phase_log_entry_to_dict() -> None:
    """Test OODAPhaseLogEntry serializes correctly."""
    from src.core.ooda import OODAPhaseLogEntry, OODAPhase

    entry = OODAPhaseLogEntry(
        phase=OODAPhase.DECIDE,
        iteration=2,
        input_summary="Analyzed patterns",
        output_summary="Selected action: research",
        tokens_used=200,
        duration_ms=150,
    )

    result = entry.to_dict()

    assert result["phase"] == "decide"
    assert result["iteration"] == 2
    assert result["tokens_used"] == 200
    assert "timestamp" in result
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_ooda.py::test_ooda_phase_enum_has_four_phases -v`
Expected: FAIL with "ModuleNotFoundError: No module named 'src.core.ooda'"

**Step 3: Write minimal implementation**

Create `backend/src/core/ooda.py`:

```python
"""OODA loop cognitive processing module.

Implements the Observe-Orient-Decide-Act loop for systematic
reasoning about tasks. The OODA loop is ARIA's core cognitive
processing framework that iterates until goals are achieved.

Phases:
- Observe: Gather context from memory and environment
- Orient: Analyze situation, identify patterns
- Decide: Select best action from options
- Act: Execute chosen action

Each phase is logged for transparency and debugging.
"""

import logging
from dataclasses import dataclass, field
from datetime import UTC, datetime
from enum import Enum
from typing import Any

logger = logging.getLogger(__name__)


class OODAPhase(Enum):
    """Phases of the OODA loop cognitive cycle."""

    OBSERVE = "observe"
    ORIENT = "orient"
    DECIDE = "decide"
    ACT = "act"


@dataclass
class OODAPhaseLogEntry:
    """Log entry for a single OODA phase execution.

    Captures execution details for transparency and debugging.
    """

    phase: OODAPhase
    iteration: int
    input_summary: str
    output_summary: str
    tokens_used: int = 0
    duration_ms: int = 0
    timestamp: datetime = field(default_factory=lambda: datetime.now(UTC))

    def to_dict(self) -> dict[str, Any]:
        """Serialize to dictionary.

        Returns:
            Dictionary representation suitable for JSON.
        """
        return {
            "phase": self.phase.value,
            "iteration": self.iteration,
            "input_summary": self.input_summary,
            "output_summary": self.output_summary,
            "tokens_used": self.tokens_used,
            "duration_ms": self.duration_ms,
            "timestamp": self.timestamp.isoformat(),
        }


@dataclass
class OODAState:
    """State of an OODA loop execution.

    Tracks the current phase, observations, analysis results,
    decisions, and action outcomes across iterations.
    """

    goal_id: str
    current_phase: OODAPhase = OODAPhase.OBSERVE
    observations: list[dict[str, Any]] = field(default_factory=list)
    orientation: dict[str, Any] = field(default_factory=dict)
    decision: dict[str, Any] | None = None
    action_result: Any = None
    iteration: int = 0
    max_iterations: int = 10
    phase_logs: list[OODAPhaseLogEntry] = field(default_factory=list)
    is_complete: bool = False
    is_blocked: bool = False
    blocked_reason: str | None = None

    def to_dict(self) -> dict[str, Any]:
        """Serialize state to dictionary.

        Returns:
            Dictionary representation suitable for JSON.
        """
        return {
            "goal_id": self.goal_id,
            "current_phase": self.current_phase.value,
            "observations": self.observations,
            "orientation": self.orientation,
            "decision": self.decision,
            "action_result": self.action_result,
            "iteration": self.iteration,
            "max_iterations": self.max_iterations,
            "phase_logs": [log.to_dict() for log in self.phase_logs],
            "is_complete": self.is_complete,
            "is_blocked": self.is_blocked,
            "blocked_reason": self.blocked_reason,
        }

    @classmethod
    def from_dict(cls, data: dict[str, Any]) -> "OODAState":
        """Create OODAState from dictionary.

        Args:
            data: Dictionary containing state data.

        Returns:
            OODAState instance with restored state.
        """
        state = cls(
            goal_id=data["goal_id"],
            current_phase=OODAPhase(data["current_phase"]),
            observations=data.get("observations", []),
            orientation=data.get("orientation", {}),
            decision=data.get("decision"),
            action_result=data.get("action_result"),
            iteration=data.get("iteration", 0),
            max_iterations=data.get("max_iterations", 10),
            is_complete=data.get("is_complete", False),
            is_blocked=data.get("is_blocked", False),
            blocked_reason=data.get("blocked_reason"),
        )

        # Restore phase logs
        for log_data in data.get("phase_logs", []):
            state.phase_logs.append(
                OODAPhaseLogEntry(
                    phase=OODAPhase(log_data["phase"]),
                    iteration=log_data["iteration"],
                    input_summary=log_data["input_summary"],
                    output_summary=log_data["output_summary"],
                    tokens_used=log_data.get("tokens_used", 0),
                    duration_ms=log_data.get("duration_ms", 0),
                    timestamp=datetime.fromisoformat(log_data["timestamp"]),
                )
            )

        return state
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_ooda.py -v`
Expected: PASS (6 tests)

**Step 5: Commit**

```bash
git add backend/src/core/ooda.py backend/tests/test_ooda.py
git commit -m "feat(ooda): add OODAPhase enum and OODAState dataclass

- OODAPhase enum with OBSERVE, ORIENT, DECIDE, ACT
- OODAState dataclass for tracking loop execution
- OODAPhaseLogEntry for phase execution logging
- Serialization support with to_dict/from_dict"
```

---

## Task 3: Create OODAConfig for Thinking Budgets

**Files:**
- Modify: `backend/src/core/ooda.py`
- Test: `backend/tests/test_ooda.py`

**Step 1: Write the failing test**

Add to `backend/tests/test_ooda.py`:

```python
def test_ooda_config_has_default_budgets() -> None:
    """Test OODAConfig has sensible default token budgets."""
    from src.core.ooda import OODAConfig

    config = OODAConfig()

    assert config.observe_budget == 2000
    assert config.orient_budget == 3000
    assert config.decide_budget == 2000
    assert config.act_budget == 1000
    assert config.max_iterations == 10
    assert config.total_budget == 50000


def test_ooda_config_custom_budgets() -> None:
    """Test OODAConfig accepts custom token budgets."""
    from src.core.ooda import OODAConfig

    config = OODAConfig(
        observe_budget=1000,
        orient_budget=1500,
        decide_budget=1000,
        act_budget=500,
        max_iterations=5,
        total_budget=20000,
    )

    assert config.observe_budget == 1000
    assert config.orient_budget == 1500
    assert config.max_iterations == 5


def test_ooda_config_to_dict() -> None:
    """Test OODAConfig serializes correctly."""
    from src.core.ooda import OODAConfig

    config = OODAConfig(max_iterations=3)
    result = config.to_dict()

    assert result["max_iterations"] == 3
    assert "observe_budget" in result
    assert "total_budget" in result
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_ooda.py::test_ooda_config_has_default_budgets -v`
Expected: FAIL with "ImportError: cannot import name 'OODAConfig'"

**Step 3: Write minimal implementation**

Add to `backend/src/core/ooda.py` (after OODAPhase enum):

```python
@dataclass
class OODAConfig:
    """Configuration for OODA loop execution.

    Controls token budgets per phase and overall constraints.
    """

    observe_budget: int = 2000  # Max tokens for observation phase
    orient_budget: int = 3000  # Max tokens for orientation phase
    decide_budget: int = 2000  # Max tokens for decision phase
    act_budget: int = 1000  # Max tokens for action phase
    max_iterations: int = 10  # Maximum OODA cycles before stopping
    total_budget: int = 50000  # Total token budget across all iterations

    def to_dict(self) -> dict[str, Any]:
        """Serialize to dictionary.

        Returns:
            Dictionary representation.
        """
        return {
            "observe_budget": self.observe_budget,
            "orient_budget": self.orient_budget,
            "decide_budget": self.decide_budget,
            "act_budget": self.act_budget,
            "max_iterations": self.max_iterations,
            "total_budget": self.total_budget,
        }
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_ooda.py -k "config" -v`
Expected: PASS (3 tests)

**Step 5: Commit**

```bash
git add backend/src/core/ooda.py backend/tests/test_ooda.py
git commit -m "feat(ooda): add OODAConfig for configurable thinking budgets

- Per-phase token budgets (observe, orient, decide, act)
- Total budget and max iterations constraints
- Sensible defaults for production use"
```

---

## Task 4: Implement OODALoop.observe() Method

**Files:**
- Modify: `backend/src/core/ooda.py`
- Test: `backend/tests/test_ooda.py`

**Step 1: Write the failing test**

Add to `backend/tests/test_ooda.py`:

```python
from unittest.mock import AsyncMock, MagicMock, patch


@pytest.mark.asyncio
async def test_ooda_loop_observe_gathers_memory_context() -> None:
    """Test observe phase queries episodic and semantic memory."""
    from src.core.ooda import OODALoop, OODAState, OODAConfig

    # Mock dependencies
    mock_llm = MagicMock()
    mock_episodic = AsyncMock()
    mock_semantic = AsyncMock()
    mock_working = MagicMock()

    # Setup mock returns
    mock_episodic.semantic_search.return_value = [
        MagicMock(
            id="ep-1",
            content="Met with John about Q3 budget",
            to_dict=lambda: {"id": "ep-1", "content": "Met with John about Q3 budget"},
        )
    ]
    mock_semantic.search_facts.return_value = [
        MagicMock(
            id="fact-1",
            subject="John",
            predicate="is",
            object="CFO",
            to_dict=lambda: {"id": "fact-1", "subject": "John", "predicate": "is", "object": "CFO"},
        )
    ]
    mock_working.get_context_for_llm.return_value = [
        {"role": "user", "content": "Help me prepare for the budget meeting"}
    ]

    loop = OODALoop(
        llm_client=mock_llm,
        episodic_memory=mock_episodic,
        semantic_memory=mock_semantic,
        working_memory=mock_working,
    )

    state = OODAState(goal_id="goal-123")
    goal = {"id": "goal-123", "title": "Prepare budget meeting", "description": "Get ready for Q3 budget review"}

    new_state = await loop.observe(state, goal)

    # Verify observations were gathered
    assert len(new_state.observations) > 0
    assert any(obs["source"] == "episodic" for obs in new_state.observations)
    assert any(obs["source"] == "semantic" for obs in new_state.observations)
    assert any(obs["source"] == "working" for obs in new_state.observations)

    # Verify memory was queried
    mock_episodic.semantic_search.assert_called_once()
    mock_semantic.search_facts.assert_called_once()


@pytest.mark.asyncio
async def test_ooda_loop_observe_logs_phase_execution() -> None:
    """Test observe phase logs its execution."""
    from src.core.ooda import OODALoop, OODAState, OODAPhase

    mock_llm = MagicMock()
    mock_episodic = AsyncMock()
    mock_semantic = AsyncMock()
    mock_working = MagicMock()

    mock_episodic.semantic_search.return_value = []
    mock_semantic.search_facts.return_value = []
    mock_working.get_context_for_llm.return_value = []

    loop = OODALoop(
        llm_client=mock_llm,
        episodic_memory=mock_episodic,
        semantic_memory=mock_semantic,
        working_memory=mock_working,
    )

    state = OODAState(goal_id="goal-123")
    goal = {"id": "goal-123", "title": "Test goal"}

    new_state = await loop.observe(state, goal)

    # Verify phase was logged
    assert len(new_state.phase_logs) == 1
    assert new_state.phase_logs[0].phase == OODAPhase.OBSERVE
    assert new_state.phase_logs[0].iteration == 0


@pytest.mark.asyncio
async def test_ooda_loop_observe_handles_memory_errors_gracefully() -> None:
    """Test observe phase handles memory query errors."""
    from src.core.ooda import OODALoop, OODAState

    mock_llm = MagicMock()
    mock_episodic = AsyncMock()
    mock_semantic = AsyncMock()
    mock_working = MagicMock()

    # Simulate memory error
    mock_episodic.semantic_search.side_effect = Exception("Connection failed")
    mock_semantic.search_facts.return_value = []
    mock_working.get_context_for_llm.return_value = []

    loop = OODALoop(
        llm_client=mock_llm,
        episodic_memory=mock_episodic,
        semantic_memory=mock_semantic,
        working_memory=mock_working,
    )

    state = OODAState(goal_id="goal-123")
    goal = {"id": "goal-123", "title": "Test goal"}

    # Should not raise, but continue with available data
    new_state = await loop.observe(state, goal)

    # Verify we got some observations (at least from working memory)
    assert new_state is not None
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_ooda.py::test_ooda_loop_observe_gathers_memory_context -v`
Expected: FAIL with "ImportError: cannot import name 'OODALoop'"

**Step 3: Write minimal implementation**

Add to `backend/src/core/ooda.py` (after OODAState class):

```python
import time
from typing import TYPE_CHECKING

if TYPE_CHECKING:
    from src.core.llm import LLMClient
    from src.memory.episodic import EpisodicMemory
    from src.memory.semantic import SemanticMemory
    from src.memory.working import WorkingMemory


class OODALoop:
    """OODA loop cognitive processing engine.

    Implements the Observe-Orient-Decide-Act cycle for systematic
    task reasoning. Each iteration gathers context, analyzes the
    situation, selects an action, and executes it.
    """

    def __init__(
        self,
        llm_client: "LLMClient",
        episodic_memory: "EpisodicMemory",
        semantic_memory: "SemanticMemory",
        working_memory: "WorkingMemory",
        config: OODAConfig | None = None,
    ) -> None:
        """Initialize OODA loop.

        Args:
            llm_client: LLM client for reasoning.
            episodic_memory: Episodic memory service.
            semantic_memory: Semantic memory service.
            working_memory: Working memory for current context.
            config: Optional configuration for budgets.
        """
        self.llm = llm_client
        self.episodic = episodic_memory
        self.semantic = semantic_memory
        self.working = working_memory
        self.config = config or OODAConfig()

    async def observe(
        self,
        state: OODAState,
        goal: dict[str, Any],
    ) -> OODAState:
        """Gather relevant information from memory and context.

        Queries episodic memory for related events, semantic memory
        for relevant facts, and working memory for current context.

        Args:
            state: Current OODA state.
            goal: The goal being pursued.

        Returns:
            Updated state with observations.
        """
        start_time = time.perf_counter()
        observations: list[dict[str, Any]] = []

        # Build search query from goal
        search_query = f"{goal.get('title', '')} {goal.get('description', '')}"

        # Get user_id from working memory
        user_id = self.working.user_id

        # Query episodic memory for related events
        try:
            episodes = await self.episodic.semantic_search(
                user_id=user_id,
                query=search_query,
                limit=5,
            )
            for episode in episodes:
                observations.append({
                    "source": "episodic",
                    "type": "episode",
                    "data": episode.to_dict() if hasattr(episode, "to_dict") else str(episode),
                })
        except Exception as e:
            logger.warning(f"Failed to query episodic memory: {e}")

        # Query semantic memory for relevant facts
        try:
            facts = await self.semantic.search_facts(
                user_id=user_id,
                query=search_query,
                limit=10,
            )
            for fact in facts:
                observations.append({
                    "source": "semantic",
                    "type": "fact",
                    "data": fact.to_dict() if hasattr(fact, "to_dict") else str(fact),
                })
        except Exception as e:
            logger.warning(f"Failed to query semantic memory: {e}")

        # Get current working memory context
        try:
            context = self.working.get_context_for_llm()
            observations.append({
                "source": "working",
                "type": "conversation",
                "data": context,
            })
        except Exception as e:
            logger.warning(f"Failed to get working memory context: {e}")

        # Update state
        state.observations = observations
        state.current_phase = OODAPhase.ORIENT

        # Log phase execution
        duration_ms = int((time.perf_counter() - start_time) * 1000)
        state.phase_logs.append(
            OODAPhaseLogEntry(
                phase=OODAPhase.OBSERVE,
                iteration=state.iteration,
                input_summary=f"Goal: {goal.get('title', 'Unknown')}",
                output_summary=f"Gathered {len(observations)} observations",
                duration_ms=duration_ms,
            )
        )

        logger.info(
            "OODA observe phase complete",
            extra={
                "goal_id": state.goal_id,
                "iteration": state.iteration,
                "observation_count": len(observations),
                "duration_ms": duration_ms,
            },
        )

        return state
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_ooda.py -k "observe" -v`
Expected: PASS (3 tests)

**Step 5: Commit**

```bash
git add backend/src/core/ooda.py backend/tests/test_ooda.py
git commit -m "feat(ooda): implement observe phase

- Query episodic memory for related events
- Query semantic memory for relevant facts
- Include working memory conversation context
- Handle memory errors gracefully
- Log phase execution with timing"
```

---

## Task 5: Implement OODALoop.orient() Method

**Files:**
- Modify: `backend/src/core/ooda.py`
- Test: `backend/tests/test_ooda.py`

**Step 1: Write the failing test**

Add to `backend/tests/test_ooda.py`:

```python
@pytest.mark.asyncio
async def test_ooda_loop_orient_analyzes_observations() -> None:
    """Test orient phase uses LLM to analyze observations."""
    from src.core.ooda import OODALoop, OODAState, OODAPhase

    mock_llm = AsyncMock()
    mock_llm.generate_response.return_value = """{
        "patterns": ["Budget meeting coming up", "John is key stakeholder"],
        "opportunities": ["Prepare talking points"],
        "threats": ["Tight deadline"],
        "recommended_focus": "Gather budget data"
    }"""

    mock_episodic = AsyncMock()
    mock_semantic = AsyncMock()
    mock_working = MagicMock()
    mock_working.user_id = "user-123"

    loop = OODALoop(
        llm_client=mock_llm,
        episodic_memory=mock_episodic,
        semantic_memory=mock_semantic,
        working_memory=mock_working,
    )

    state = OODAState(goal_id="goal-123")
    state.observations = [
        {"source": "episodic", "type": "episode", "data": {"content": "Q3 budget meeting scheduled"}},
        {"source": "semantic", "type": "fact", "data": {"subject": "John", "predicate": "is", "object": "CFO"}},
    ]
    goal = {"id": "goal-123", "title": "Prepare budget meeting"}

    new_state = await loop.orient(state, goal)

    # Verify orientation was produced
    assert new_state.orientation is not None
    assert "patterns" in new_state.orientation
    assert new_state.current_phase == OODAPhase.DECIDE

    # Verify LLM was called
    mock_llm.generate_response.assert_called_once()


@pytest.mark.asyncio
async def test_ooda_loop_orient_logs_phase_execution() -> None:
    """Test orient phase logs its execution."""
    from src.core.ooda import OODALoop, OODAState, OODAPhase

    mock_llm = AsyncMock()
    mock_llm.generate_response.return_value = '{"patterns": [], "opportunities": [], "threats": [], "recommended_focus": "test"}'

    mock_episodic = AsyncMock()
    mock_semantic = AsyncMock()
    mock_working = MagicMock()
    mock_working.user_id = "user-123"

    loop = OODALoop(
        llm_client=mock_llm,
        episodic_memory=mock_episodic,
        semantic_memory=mock_semantic,
        working_memory=mock_working,
    )

    state = OODAState(goal_id="goal-123")
    state.observations = []
    goal = {"id": "goal-123", "title": "Test goal"}

    new_state = await loop.orient(state, goal)

    # Find orient phase log
    orient_logs = [log for log in new_state.phase_logs if log.phase == OODAPhase.ORIENT]
    assert len(orient_logs) == 1


@pytest.mark.asyncio
async def test_ooda_loop_orient_handles_invalid_llm_response() -> None:
    """Test orient phase handles malformed LLM response."""
    from src.core.ooda import OODALoop, OODAState

    mock_llm = AsyncMock()
    mock_llm.generate_response.return_value = "This is not valid JSON"

    mock_episodic = AsyncMock()
    mock_semantic = AsyncMock()
    mock_working = MagicMock()
    mock_working.user_id = "user-123"

    loop = OODALoop(
        llm_client=mock_llm,
        episodic_memory=mock_episodic,
        semantic_memory=mock_semantic,
        working_memory=mock_working,
    )

    state = OODAState(goal_id="goal-123")
    state.observations = []
    goal = {"id": "goal-123", "title": "Test goal"}

    # Should not raise, but produce default orientation
    new_state = await loop.orient(state, goal)

    assert new_state.orientation is not None
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_ooda.py::test_ooda_loop_orient_analyzes_observations -v`
Expected: FAIL with "AttributeError: 'OODALoop' object has no attribute 'orient'"

**Step 3: Write minimal implementation**

Add to `OODALoop` class in `backend/src/core/ooda.py`:

```python
    async def orient(
        self,
        state: OODAState,
        goal: dict[str, Any],
    ) -> OODAState:
        """Analyze observations and identify patterns.

        Uses LLM to synthesize observations, identify threats and
        opportunities, and map to available agent capabilities.

        Args:
            state: Current OODA state with observations.
            goal: The goal being pursued.

        Returns:
            Updated state with orientation analysis.
        """
        import json

        start_time = time.perf_counter()

        # Build prompt for LLM analysis
        observations_summary = json.dumps(state.observations, indent=2, default=str)

        system_prompt = """You are ARIA's cognitive analysis module. Analyze the observations and produce a structured analysis.

Output ONLY valid JSON with this structure:
{
    "patterns": ["list of patterns identified"],
    "opportunities": ["list of opportunities to pursue the goal"],
    "threats": ["list of obstacles or risks"],
    "recommended_focus": "single most important area to focus on"
}"""

        user_prompt = f"""Goal: {goal.get('title', 'Unknown')}
Description: {goal.get('description', 'No description')}

Observations:
{observations_summary}

Analyze these observations and identify patterns, opportunities, and threats relevant to achieving the goal."""

        # Call LLM for analysis
        try:
            response = await self.llm.generate_response(
                messages=[{"role": "user", "content": user_prompt}],
                system_prompt=system_prompt,
                max_tokens=self.config.orient_budget,
                temperature=0.3,  # Lower temperature for more focused analysis
            )

            # Parse JSON response
            try:
                orientation = json.loads(response)
            except json.JSONDecodeError:
                logger.warning("Failed to parse LLM response as JSON, using default orientation")
                orientation = {
                    "patterns": [],
                    "opportunities": [],
                    "threats": [],
                    "recommended_focus": "Unable to analyze - proceeding with default strategy",
                    "raw_response": response,
                }

        except Exception as e:
            logger.error(f"LLM call failed in orient phase: {e}")
            orientation = {
                "patterns": [],
                "opportunities": [],
                "threats": [],
                "recommended_focus": "LLM analysis failed - proceeding cautiously",
                "error": str(e),
            }

        # Update state
        state.orientation = orientation
        state.current_phase = OODAPhase.DECIDE

        # Log phase execution
        duration_ms = int((time.perf_counter() - start_time) * 1000)
        state.phase_logs.append(
            OODAPhaseLogEntry(
                phase=OODAPhase.ORIENT,
                iteration=state.iteration,
                input_summary=f"Analyzed {len(state.observations)} observations",
                output_summary=f"Focus: {orientation.get('recommended_focus', 'Unknown')}",
                duration_ms=duration_ms,
            )
        )

        logger.info(
            "OODA orient phase complete",
            extra={
                "goal_id": state.goal_id,
                "iteration": state.iteration,
                "patterns_found": len(orientation.get("patterns", [])),
                "duration_ms": duration_ms,
            },
        )

        return state
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_ooda.py -k "orient" -v`
Expected: PASS (3 tests)

**Step 5: Commit**

```bash
git add backend/src/core/ooda.py backend/tests/test_ooda.py
git commit -m "feat(ooda): implement orient phase

- Use LLM to analyze observations
- Identify patterns, opportunities, threats
- Produce recommended focus area
- Handle invalid LLM responses gracefully
- Log phase execution with timing"
```

---

## Task 6: Implement OODALoop.decide() Method

**Files:**
- Modify: `backend/src/core/ooda.py`
- Test: `backend/tests/test_ooda.py`

**Step 1: Write the failing test**

Add to `backend/tests/test_ooda.py`:

```python
@pytest.mark.asyncio
async def test_ooda_loop_decide_selects_action() -> None:
    """Test decide phase uses LLM to select best action."""
    from src.core.ooda import OODALoop, OODAState, OODAPhase

    mock_llm = AsyncMock()
    mock_llm.generate_response.return_value = """{
        "action": "research",
        "agent": "analyst",
        "parameters": {"query": "Q3 budget data"},
        "reasoning": "Need budget data before meeting"
    }"""

    mock_episodic = AsyncMock()
    mock_semantic = AsyncMock()
    mock_working = MagicMock()
    mock_working.user_id = "user-123"

    loop = OODALoop(
        llm_client=mock_llm,
        episodic_memory=mock_episodic,
        semantic_memory=mock_semantic,
        working_memory=mock_working,
    )

    state = OODAState(goal_id="goal-123")
    state.orientation = {
        "patterns": ["Budget meeting upcoming"],
        "opportunities": ["Prepare data"],
        "threats": ["Time pressure"],
        "recommended_focus": "Gather budget data",
    }
    goal = {"id": "goal-123", "title": "Prepare budget meeting"}

    new_state = await loop.decide(state, goal)

    # Verify decision was made
    assert new_state.decision is not None
    assert "action" in new_state.decision
    assert new_state.current_phase == OODAPhase.ACT

    # Verify LLM was called
    mock_llm.generate_response.assert_called_once()


@pytest.mark.asyncio
async def test_ooda_loop_decide_can_mark_goal_complete() -> None:
    """Test decide phase can recognize goal is achieved."""
    from src.core.ooda import OODALoop, OODAState

    mock_llm = AsyncMock()
    mock_llm.generate_response.return_value = """{
        "action": "complete",
        "agent": null,
        "parameters": {},
        "reasoning": "All objectives have been met"
    }"""

    mock_episodic = AsyncMock()
    mock_semantic = AsyncMock()
    mock_working = MagicMock()
    mock_working.user_id = "user-123"

    loop = OODALoop(
        llm_client=mock_llm,
        episodic_memory=mock_episodic,
        semantic_memory=mock_semantic,
        working_memory=mock_working,
    )

    state = OODAState(goal_id="goal-123")
    state.orientation = {"recommended_focus": "Review completion"}
    goal = {"id": "goal-123", "title": "Completed goal"}

    new_state = await loop.decide(state, goal)

    assert new_state.decision["action"] == "complete"
    assert new_state.is_complete is True


@pytest.mark.asyncio
async def test_ooda_loop_decide_can_mark_blocked() -> None:
    """Test decide phase can recognize goal is blocked."""
    from src.core.ooda import OODALoop, OODAState

    mock_llm = AsyncMock()
    mock_llm.generate_response.return_value = """{
        "action": "blocked",
        "agent": null,
        "parameters": {},
        "reasoning": "Missing required permissions"
    }"""

    mock_episodic = AsyncMock()
    mock_semantic = AsyncMock()
    mock_working = MagicMock()
    mock_working.user_id = "user-123"

    loop = OODALoop(
        llm_client=mock_llm,
        episodic_memory=mock_episodic,
        semantic_memory=mock_semantic,
        working_memory=mock_working,
    )

    state = OODAState(goal_id="goal-123")
    state.orientation = {"threats": ["No access"]}
    goal = {"id": "goal-123", "title": "Blocked goal"}

    new_state = await loop.decide(state, goal)

    assert new_state.decision["action"] == "blocked"
    assert new_state.is_blocked is True
    assert "permissions" in new_state.blocked_reason.lower()
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_ooda.py::test_ooda_loop_decide_selects_action -v`
Expected: FAIL with "AttributeError: 'OODALoop' object has no attribute 'decide'"

**Step 3: Write minimal implementation**

Add to `OODALoop` class in `backend/src/core/ooda.py`:

```python
    async def decide(
        self,
        state: OODAState,
        goal: dict[str, Any],
    ) -> OODAState:
        """Select the best action to take.

        Uses LLM to generate action options, evaluate them against
        the goal, and select the highest-value action.

        Args:
            state: Current OODA state with orientation.
            goal: The goal being pursued.

        Returns:
            Updated state with decision.
        """
        import json

        start_time = time.perf_counter()

        # Build prompt for decision
        orientation_summary = json.dumps(state.orientation, indent=2, default=str)

        system_prompt = """You are ARIA's decision module. Based on the analysis, select the best action to take.

Available actions:
- "research": Use Analyst agent to gather information
- "search": Use Hunter agent to find leads/companies
- "communicate": Use Scribe agent to draft communications
- "schedule": Use Operator agent for calendar/CRM operations
- "monitor": Use Scout agent for intelligence gathering
- "plan": Use Strategist agent for planning
- "complete": Goal has been achieved
- "blocked": Cannot proceed without user intervention

Output ONLY valid JSON with this structure:
{
    "action": "action_name",
    "agent": "agent_type or null",
    "parameters": {"key": "value"},
    "reasoning": "why this action was chosen"
}"""

        user_prompt = f"""Goal: {goal.get('title', 'Unknown')}
Description: {goal.get('description', 'No description')}

Analysis:
{orientation_summary}

Iteration: {state.iteration + 1} of {state.max_iterations}

Previous action result: {state.action_result if state.action_result else 'No previous action'}

Select the best action to make progress toward the goal."""

        # Call LLM for decision
        try:
            response = await self.llm.generate_response(
                messages=[{"role": "user", "content": user_prompt}],
                system_prompt=system_prompt,
                max_tokens=self.config.decide_budget,
                temperature=0.2,  # Low temperature for more deterministic decisions
            )

            # Parse JSON response
            try:
                decision = json.loads(response)
            except json.JSONDecodeError:
                logger.warning("Failed to parse LLM decision as JSON")
                decision = {
                    "action": "blocked",
                    "agent": None,
                    "parameters": {},
                    "reasoning": "Failed to parse decision",
                    "raw_response": response,
                }

        except Exception as e:
            logger.error(f"LLM call failed in decide phase: {e}")
            decision = {
                "action": "blocked",
                "agent": None,
                "parameters": {},
                "reasoning": f"Decision failed: {e}",
                "error": str(e),
            }

        # Update state based on decision
        state.decision = decision

        if decision.get("action") == "complete":
            state.is_complete = True
        elif decision.get("action") == "blocked":
            state.is_blocked = True
            state.blocked_reason = decision.get("reasoning", "Unknown reason")
        else:
            state.current_phase = OODAPhase.ACT

        # Log phase execution
        duration_ms = int((time.perf_counter() - start_time) * 1000)
        state.phase_logs.append(
            OODAPhaseLogEntry(
                phase=OODAPhase.DECIDE,
                iteration=state.iteration,
                input_summary=f"Focus: {state.orientation.get('recommended_focus', 'Unknown')}",
                output_summary=f"Decision: {decision.get('action', 'Unknown')}",
                duration_ms=duration_ms,
            )
        )

        logger.info(
            "OODA decide phase complete",
            extra={
                "goal_id": state.goal_id,
                "iteration": state.iteration,
                "action": decision.get("action"),
                "agent": decision.get("agent"),
                "duration_ms": duration_ms,
            },
        )

        return state
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_ooda.py -k "decide" -v`
Expected: PASS (3 tests)

**Step 5: Commit**

```bash
git add backend/src/core/ooda.py backend/tests/test_ooda.py
git commit -m "feat(ooda): implement decide phase

- Use LLM to select best action from options
- Support for complete and blocked states
- Available actions map to agent types
- Handle invalid LLM responses gracefully
- Log phase execution with timing"
```

---

## Task 7: Implement OODALoop.act() Method

**Files:**
- Modify: `backend/src/core/ooda.py`
- Test: `backend/tests/test_ooda.py`

**Step 1: Write the failing test**

Add to `backend/tests/test_ooda.py`:

```python
@pytest.mark.asyncio
async def test_ooda_loop_act_executes_decision() -> None:
    """Test act phase executes the decided action."""
    from src.core.ooda import OODALoop, OODAState, OODAPhase

    mock_llm = MagicMock()
    mock_episodic = AsyncMock()
    mock_semantic = AsyncMock()
    mock_working = MagicMock()
    mock_working.user_id = "user-123"

    # Create a mock agent executor
    mock_executor = AsyncMock()
    mock_executor.return_value = {
        "success": True,
        "data": {"results": ["Budget data found"]},
    }

    loop = OODALoop(
        llm_client=mock_llm,
        episodic_memory=mock_episodic,
        semantic_memory=mock_semantic,
        working_memory=mock_working,
    )
    loop.agent_executor = mock_executor

    state = OODAState(goal_id="goal-123")
    state.decision = {
        "action": "research",
        "agent": "analyst",
        "parameters": {"query": "Q3 budget data"},
        "reasoning": "Need budget data",
    }
    goal = {"id": "goal-123", "title": "Prepare budget meeting"}

    new_state = await loop.act(state, goal)

    # Verify action was executed
    assert new_state.action_result is not None
    assert new_state.action_result["success"] is True
    assert new_state.current_phase == OODAPhase.OBSERVE  # Loops back
    assert new_state.iteration == 1  # Incremented


@pytest.mark.asyncio
async def test_ooda_loop_act_skips_complete_state() -> None:
    """Test act phase does nothing if goal is complete."""
    from src.core.ooda import OODALoop, OODAState

    mock_llm = MagicMock()
    mock_episodic = AsyncMock()
    mock_semantic = AsyncMock()
    mock_working = MagicMock()
    mock_working.user_id = "user-123"

    mock_executor = AsyncMock()

    loop = OODALoop(
        llm_client=mock_llm,
        episodic_memory=mock_episodic,
        semantic_memory=mock_semantic,
        working_memory=mock_working,
    )
    loop.agent_executor = mock_executor

    state = OODAState(goal_id="goal-123")
    state.decision = {"action": "complete"}
    state.is_complete = True
    goal = {"id": "goal-123", "title": "Complete goal"}

    new_state = await loop.act(state, goal)

    # Agent executor should not be called
    mock_executor.assert_not_called()
    assert new_state.is_complete is True


@pytest.mark.asyncio
async def test_ooda_loop_act_handles_execution_failure() -> None:
    """Test act phase handles agent execution failure."""
    from src.core.ooda import OODALoop, OODAState, OODAPhase

    mock_llm = MagicMock()
    mock_episodic = AsyncMock()
    mock_semantic = AsyncMock()
    mock_working = MagicMock()
    mock_working.user_id = "user-123"

    mock_executor = AsyncMock()
    mock_executor.side_effect = Exception("Agent failed")

    loop = OODALoop(
        llm_client=mock_llm,
        episodic_memory=mock_episodic,
        semantic_memory=mock_semantic,
        working_memory=mock_working,
    )
    loop.agent_executor = mock_executor

    state = OODAState(goal_id="goal-123")
    state.decision = {
        "action": "research",
        "agent": "analyst",
        "parameters": {},
    }
    goal = {"id": "goal-123", "title": "Test goal"}

    new_state = await loop.act(state, goal)

    # Should record failure but continue
    assert new_state.action_result is not None
    assert new_state.action_result["success"] is False
    assert "error" in new_state.action_result
    assert new_state.current_phase == OODAPhase.OBSERVE  # Still loops back
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_ooda.py::test_ooda_loop_act_executes_decision -v`
Expected: FAIL with "AttributeError: 'OODALoop' object has no attribute 'act'"

**Step 3: Write minimal implementation**

Add to `OODALoop` class in `backend/src/core/ooda.py`:

```python
    async def act(
        self,
        state: OODAState,
        goal: dict[str, Any],
    ) -> OODAState:
        """Execute the decided action.

        Dispatches to the appropriate agent and records the result.
        Increments iteration and loops back to observe phase.

        Args:
            state: Current OODA state with decision.
            goal: The goal being pursued.

        Returns:
            Updated state with action result.
        """
        start_time = time.perf_counter()

        # Skip execution if goal is complete or blocked
        if state.is_complete or state.is_blocked:
            logger.info(
                "OODA act phase skipped",
                extra={
                    "goal_id": state.goal_id,
                    "is_complete": state.is_complete,
                    "is_blocked": state.is_blocked,
                },
            )
            return state

        decision = state.decision or {}
        action = decision.get("action", "unknown")
        agent = decision.get("agent")
        parameters = decision.get("parameters", {})

        # Execute action via agent executor
        try:
            if hasattr(self, "agent_executor") and self.agent_executor:
                result = await self.agent_executor(
                    action=action,
                    agent=agent,
                    parameters=parameters,
                    goal=goal,
                )
                state.action_result = result
            else:
                # No agent executor configured - record as pending
                state.action_result = {
                    "success": False,
                    "pending": True,
                    "message": "Agent executor not configured",
                    "action": action,
                    "agent": agent,
                }

        except Exception as e:
            logger.error(f"Agent execution failed: {e}")
            state.action_result = {
                "success": False,
                "error": str(e),
                "action": action,
                "agent": agent,
            }

        # Loop back to observe phase and increment iteration
        state.current_phase = OODAPhase.OBSERVE
        state.iteration += 1

        # Log phase execution
        duration_ms = int((time.perf_counter() - start_time) * 1000)
        success = state.action_result.get("success", False) if state.action_result else False
        state.phase_logs.append(
            OODAPhaseLogEntry(
                phase=OODAPhase.ACT,
                iteration=state.iteration - 1,  # Log for previous iteration
                input_summary=f"Action: {action}, Agent: {agent}",
                output_summary=f"Success: {success}",
                duration_ms=duration_ms,
            )
        )

        logger.info(
            "OODA act phase complete",
            extra={
                "goal_id": state.goal_id,
                "iteration": state.iteration,
                "action": action,
                "agent": agent,
                "success": success,
                "duration_ms": duration_ms,
            },
        )

        return state
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_ooda.py -k "act" -v`
Expected: PASS (3 tests)

**Step 5: Commit**

```bash
git add backend/src/core/ooda.py backend/tests/test_ooda.py
git commit -m "feat(ooda): implement act phase

- Execute decided action via agent executor
- Skip execution for complete/blocked states
- Handle execution failures gracefully
- Increment iteration and loop back to observe
- Log phase execution with timing"
```

---

## Task 8: Implement OODALoop.run() Method

**Files:**
- Modify: `backend/src/core/ooda.py`
- Test: `backend/tests/test_ooda.py`

**Step 1: Write the failing test**

Add to `backend/tests/test_ooda.py`:

```python
@pytest.mark.asyncio
async def test_ooda_loop_run_executes_full_cycle() -> None:
    """Test run method executes complete OODA cycle."""
    from src.core.ooda import OODALoop, OODAConfig

    mock_llm = AsyncMock()
    # First call to orient
    mock_llm.generate_response.side_effect = [
        '{"patterns": [], "opportunities": [], "threats": [], "recommended_focus": "test"}',
        '{"action": "complete", "agent": null, "parameters": {}, "reasoning": "Done"}',
    ]

    mock_episodic = AsyncMock()
    mock_episodic.semantic_search.return_value = []

    mock_semantic = AsyncMock()
    mock_semantic.search_facts.return_value = []

    mock_working = MagicMock()
    mock_working.user_id = "user-123"
    mock_working.get_context_for_llm.return_value = []

    loop = OODALoop(
        llm_client=mock_llm,
        episodic_memory=mock_episodic,
        semantic_memory=mock_semantic,
        working_memory=mock_working,
        config=OODAConfig(max_iterations=5),
    )

    goal = {"id": "goal-123", "title": "Test goal", "description": "A test goal"}

    final_state = await loop.run(goal)

    assert final_state.is_complete is True
    assert final_state.iteration <= 5


@pytest.mark.asyncio
async def test_ooda_loop_run_stops_at_max_iterations() -> None:
    """Test run method stops at max iterations."""
    from src.core.ooda import OODALoop, OODAConfig
    from src.core.exceptions import OODAMaxIterationsError

    mock_llm = AsyncMock()
    # Always return continue action
    mock_llm.generate_response.return_value = '{"action": "research", "agent": "analyst", "parameters": {}, "reasoning": "More research needed"}'

    mock_episodic = AsyncMock()
    mock_episodic.semantic_search.return_value = []

    mock_semantic = AsyncMock()
    mock_semantic.search_facts.return_value = []

    mock_working = MagicMock()
    mock_working.user_id = "user-123"
    mock_working.get_context_for_llm.return_value = []

    # Mock agent executor to always succeed
    mock_executor = AsyncMock()
    mock_executor.return_value = {"success": True, "data": {}}

    loop = OODALoop(
        llm_client=mock_llm,
        episodic_memory=mock_episodic,
        semantic_memory=mock_semantic,
        working_memory=mock_working,
        config=OODAConfig(max_iterations=2),
    )
    loop.agent_executor = mock_executor

    goal = {"id": "goal-123", "title": "Infinite goal"}

    with pytest.raises(OODAMaxIterationsError) as exc_info:
        await loop.run(goal)

    assert exc_info.value.details["iterations"] == 2


@pytest.mark.asyncio
async def test_ooda_loop_run_stops_when_blocked() -> None:
    """Test run method stops when blocked."""
    from src.core.ooda import OODALoop, OODAConfig
    from src.core.exceptions import OODABlockedError

    mock_llm = AsyncMock()
    mock_llm.generate_response.side_effect = [
        '{"patterns": [], "opportunities": [], "threats": [], "recommended_focus": "test"}',
        '{"action": "blocked", "agent": null, "parameters": {}, "reasoning": "No access"}',
    ]

    mock_episodic = AsyncMock()
    mock_episodic.semantic_search.return_value = []

    mock_semantic = AsyncMock()
    mock_semantic.search_facts.return_value = []

    mock_working = MagicMock()
    mock_working.user_id = "user-123"
    mock_working.get_context_for_llm.return_value = []

    loop = OODALoop(
        llm_client=mock_llm,
        episodic_memory=mock_episodic,
        semantic_memory=mock_semantic,
        working_memory=mock_working,
        config=OODAConfig(max_iterations=5),
    )

    goal = {"id": "goal-123", "title": "Blocked goal"}

    with pytest.raises(OODABlockedError) as exc_info:
        await loop.run(goal)

    assert "No access" in str(exc_info.value)
```

**Step 2: Run test to verify it fails**

Run: `cd backend && pytest tests/test_ooda.py::test_ooda_loop_run_executes_full_cycle -v`
Expected: FAIL with "AttributeError: 'OODALoop' object has no attribute 'run'"

**Step 3: Write minimal implementation**

Add to `OODALoop` class in `backend/src/core/ooda.py`:

```python
    async def run(
        self,
        goal: dict[str, Any],
    ) -> OODAState:
        """Execute full OODA loop until goal achieved.

        Iterates through observe-orient-decide-act cycles until:
        - Goal is achieved (is_complete = True)
        - Goal is blocked (is_blocked = True)
        - Maximum iterations reached

        Args:
            goal: The goal to pursue.

        Returns:
            Final OODA state.

        Raises:
            OODAMaxIterationsError: If max iterations exceeded.
            OODABlockedError: If goal becomes blocked.
        """
        from src.core.exceptions import OODABlockedError, OODAMaxIterationsError

        goal_id = goal.get("id", "unknown")
        state = OODAState(
            goal_id=goal_id,
            max_iterations=self.config.max_iterations,
        )

        logger.info(
            "Starting OODA loop",
            extra={
                "goal_id": goal_id,
                "max_iterations": state.max_iterations,
            },
        )

        while state.iteration < state.max_iterations:
            # Execute OODA cycle
            state = await self.observe(state, goal)
            state = await self.orient(state, goal)
            state = await self.decide(state, goal)

            # Check for completion or blocked before acting
            if state.is_complete:
                logger.info(
                    "OODA loop completed - goal achieved",
                    extra={"goal_id": goal_id, "iterations": state.iteration},
                )
                return state

            if state.is_blocked:
                logger.warning(
                    "OODA loop blocked",
                    extra={
                        "goal_id": goal_id,
                        "reason": state.blocked_reason,
                        "iterations": state.iteration,
                    },
                )
                raise OODABlockedError(goal_id=goal_id, reason=state.blocked_reason or "Unknown")

            # Execute action
            state = await self.act(state, goal)

        # Max iterations exceeded
        logger.warning(
            "OODA loop exceeded max iterations",
            extra={"goal_id": goal_id, "iterations": state.iteration},
        )
        raise OODAMaxIterationsError(goal_id=goal_id, iterations=state.iteration)
```

**Step 4: Run test to verify it passes**

Run: `cd backend && pytest tests/test_ooda.py -k "run" -v`
Expected: PASS (3 tests)

**Step 5: Commit**

```bash
git add backend/src/core/ooda.py backend/tests/test_ooda.py
git commit -m "feat(ooda): implement run method for full OODA cycle

- Execute observe-orient-decide-act cycles
- Stop on goal completion
- Raise OODABlockedError when blocked
- Raise OODAMaxIterationsError when limit exceeded
- Log loop progress and outcomes"
```

---

## Task 9: Run Quality Gates and Final Verification

**Files:**
- All modified files

**Step 1: Run type checking**

Run: `cd backend && mypy src/core/ooda.py --strict`
Expected: No errors

**Step 2: Run linting**

Run: `cd backend && ruff check src/core/ooda.py`
Expected: No errors

**Step 3: Run formatting check**

Run: `cd backend && ruff format src/core/ooda.py --check`
Expected: No changes needed (or apply if needed)

**Step 4: Run all OODA tests**

Run: `cd backend && pytest tests/test_ooda.py -v`
Expected: All tests pass

**Step 5: Run full test suite**

Run: `cd backend && pytest tests/ -v --ignore=tests/integration`
Expected: All tests pass

**Step 6: Commit final state**

```bash
git add -A
git commit -m "test(ooda): verify quality gates pass for OODA implementation

All mypy, ruff, and pytest checks pass."
```

---

## Task 10: Update Module Exports

**Files:**
- Modify: `backend/src/core/__init__.py` (create if needed)

**Step 1: Check if __init__.py exists**

Run: `ls backend/src/core/__init__.py`

**Step 2: Create or update __init__.py**

Create `backend/src/core/__init__.py`:

```python
"""Core modules for ARIA backend.

This package contains the core infrastructure components including
configuration, LLM client, exceptions, and OODA loop processing.
"""

from src.core.config import settings
from src.core.exceptions import (
    ARIAException,
    AuthenticationError,
    AuthorizationError,
    NotFoundError,
    OODABlockedError,
    OODALoopError,
    OODAMaxIterationsError,
    ValidationError,
)
from src.core.llm import LLMClient
from src.core.ooda import OODAConfig, OODALoop, OODAPhase, OODAPhaseLogEntry, OODAState

__all__ = [
    "settings",
    "ARIAException",
    "AuthenticationError",
    "AuthorizationError",
    "NotFoundError",
    "OODABlockedError",
    "OODALoopError",
    "OODAMaxIterationsError",
    "ValidationError",
    "LLMClient",
    "OODAConfig",
    "OODALoop",
    "OODAPhase",
    "OODAPhaseLogEntry",
    "OODAState",
]
```

**Step 3: Commit**

```bash
git add backend/src/core/__init__.py
git commit -m "feat(core): export OODA loop components from core module

Makes OODALoop, OODAState, OODAConfig, etc. importable from src.core"
```

---

## Summary

This plan implements US-301: OODA Loop Implementation with:

1. **Exception types** for OODA-specific errors
2. **OODAPhase enum** for the four cognitive phases
3. **OODAState dataclass** for tracking loop execution
4. **OODAConfig** for configurable thinking budgets
5. **OODALoop class** with async methods for each phase:
   - `observe()`: Gather context from memory
   - `orient()`: Analyze observations with LLM
   - `decide()`: Select action with LLM
   - `act()`: Execute via agent executor
   - `run()`: Full loop execution
6. **Phase logging** for transparency
7. **Comprehensive tests** following TDD

The implementation follows ARIA's coding patterns:
- Type hints throughout
- Async/await for all I/O
- Proper error handling
- Structured logging
- Dataclass serialization

---

**Plan complete and saved to `docs/plans/2026-02-02-us-301-ooda-loop.md`. Two execution options:**

**1. Subagent-Driven (this session)** - I dispatch fresh subagent per task, review between tasks, fast iteration

**2. Parallel Session (separate)** - Open new session with executing-plans, batch execution with checkpoints

**Which approach?**
